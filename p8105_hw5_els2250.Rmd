---
title: "Homework 5"
author: "Emma Sexton <br>"
date: "Due 16 November 2022"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(readxl)
library(rvest)
library(patchwork)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 8,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Import data using `list.files`

```{r, message = FALSE}
longitudinal_study_df =
  tibble(
    file_names = list.files(path = "data/problem_1/", all.files = TRUE, no.. = TRUE),
    path = str_c("data/problem_1/", file_names)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```


### Tidy data

```{r}
tidy_longitudinal_df = 
  longitudinal_study_df %>% 
  mutate(
    files = str_replace(file_names, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>%
  select(group, subject = files, week, outcome)
```

### Plot showing observations on each subject over time

```{r}
tidy_longitudinal_df %>% 
  ggplot(aes(x = week, y = outcome, group = subject, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

There appears to be a positive trend in the experimental group where outcome values increase as time passes. This trend is not observed or is not as apparent in the control group. Additionally, the values in Week 8 are higher in the experimental group compared to the control, while values were similar in both groups in Week 1. 


## Problem 2

### Importing and describing the data
```{r, message = FALSE}
homicide_df <- 
  read_csv(
    'data/problem_2/homicide-data.csv') %>%  
  janitor::clean_names()

skimr::skim(homicide_df)
```

**Summary of Dataset**: The above dataset (`homicide_df`) consists of `r nrow(homicide_df)` observations and `r ncol(homicide_df)` variables. The dataset details case information, such as case ID (`uid`) and the date the homicide was reported (`reported_date`). Additionally, information related to the victim (`victim_last`, `victim_first`, `victim_race`, `victim_age`, `victim_sex`) is detailed, as well as the location of the homicide (`city`, `state`, `lat`, `lon`). Lastly, a categorical variable (`disposition`) consists of three levels to detail the results of the homicide (i.e., "Closed without arrest", "Closed with arrest", and "Open/No arrest"). There is no missing data except for `r sum(is.na(homicide_df$lat))` missing values in `lat` and `r sum(is.na(homicide_df$lat))` missing values in `lon`. 

It's important to note that there are 51 cities depicted in the dataset currently, rather than the 50 that were indicated originally. This discrepancy is most likely related to the city Tulsa being listed in two states (Alabama and Oklahoma). There is only one homicide reported for Tulsa, AL, which suggests that this entry was a data entry error. Since we don't want to make any assumptions regarding the homicidal data, Tulsa, AL will be removed from future analyses. 


### Summarizing total number of homicides and number of unsolved homicides

To summarize the homicide data, we will generate a new variable (`city_state`) and count the total number of homicides reported in each city, as well as the number of unsolved homicides, determined by the `disposition` variable (i.e., "Closed without arrest" or "Open/No arrest"). 

```{r}
homicide_summary <- 
  homicide_df %>% 
  mutate(
    city_state = str_c(city, sep = ", ", state),
  ) %>% 
  filter(city_state != "Tulsa, AL") %>% 
  group_by(city_state) %>% 
  summarise(
    n_homs = n(),
    n_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  ) %>% 
  arrange(desc(n_homs))

homicide_summary %>% 
  knitr::kable(col.names = c("City, State", "Total Homicides (n)", "Unsolved Homicides(n)"))
```

The above dataset (`homicide_summary`) details the total number of homicides and the number of unsolved homicides that have occurred in each city. Arranging the data based on the number of homicides demonstrates that Chicago, IL, Philadelphia, PA, and Houston, TX have the highest number of total homicides. 


### Estimating proportions of unsolved homicides: Running `prop.test` for Baltimore, MD

```{r}
baltimore_hom <-
  homicide_summary %>% 
  filter(city_state == "Baltimore, MD")

baltimore_prop <-
  prop.test(
    x = baltimore_hom$n_unsolved, 
    n = baltimore_hom$n_homs
  ) %>% 
  broom::tidy() %>% 
  select(estimate, starts_with("conf"))

baltimore_prop %>% 
  knitr::kable(digits = 3, 
               col.names = c("Estimate", "Lower CI", "Upper CI"))
```

The above dataset (`baltimore_prop`) depicts the proportion of unsolved homicides in Baltimore, MD in addition to the estimate's upper and lower limit of the 95% confidence interval. The proportion of unsolved homicides in Baltimore is `r round(baltimore_prop$estimate, 3)` (95% CI: `r round(baltimore_prop$conf.low, 3)`-`r round(baltimore_prop$conf.high, 3)`). 


### Estimating Proportions of unsolved homicides: Running `prop.test` by city

```{r}
city_state_hom_prop <- homicide_summary %>% 
  mutate(
    results = map2(.x = homicide_summary$n_unsolved, 
                   .y = homicide_summary$n_homs, 
                   ~prop.test(x = .x, n = .y, conf.level = 0.95)),
    results = map(.x = results, 
                  ~broom::tidy(.x))
    ) %>% 
  unnest(results) %>% 
  select(city_state, estimate, starts_with("conf")) %>% 
  arrange(desc(estimate))

city_state_hom_prop %>% 
  knitr::kable(digits = 3, 
               col.names = c("City, State", "Estimate", "Lower CI", "Upper CI"))
```

The above dataset (`city_state_hom_prop`) depicts the estimated proportion of unsolved homicides, as well as each estimate's 95% confidence interval for each city. By arranging the dataset in descending order, it is apparent that Chicago, IL has the highest proportion of unsolved homicides, followed by New Orleans, LA and Baltimore, MD.  


### Plot of unsolved homicide proportions and confidence intervals by city

```{r}
city_state_hom_prop %>% 
  mutate(
    city_state = fct_reorder(city_state, -estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate, color = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimated Proportion of Unsolved Homicides Across the United States",
    x = "City, State",
    y = "Estimated Proportion"
  ) + 
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 60, hjust = 1),
        legend.position = 'right')
```

The above plot demonstrates the estimated proportion of unsolved homicides in each city, with its corresponding confidence interval demonstrated using error bars. As noted in previous analyses, Chicago, IL has the highest proportion of unsolved homicides, while Tulsa, AL has the lowest proportion. 



## Problem 3

### Conducting a simulation to explore power in a one-sample t-test

First, we need to set the following design elements:

```{r}
sim_model <- function(sample_n = 30, mu, sigma = 5) {

  model_df =
    tibble(
      x = rnorm(n = sample_n, mean = mu, sd = sigma),
    )

  model_df %>%
    t.test(., conf.level = 0.95) %>%
    broom::tidy() %>%
    select(estimate, p.value)

}

problem_3_df <-
  5000 %>%
  rerun(sim_model(mu = 0))
```

The above dataframe consists of 5000 datasets, where each list contains the estimate value and the p-value for that dataset when **mu = 0**. 


Next, we'll repeat this when mu = {1 ,2, 3, 4, 5, 6}:

```{r, cache = TRUE}
mu_list =
  list(
    "mu = 1" = 1,
    "mu = 2" = 2,
    "mu = 3" = 3,
    "mu = 4" = 4,
    "mu = 5" = 5,
    "mu = 6" = 6
  )

results = vector("list", length = 6)

results[[1]] = sim_model(mu = mu_list[[1]])

for (i in 1:6) {

  results[[i]] = 
    rerun(5000, sim_model(mu = mu_list[[i]])) %>% 
    bind_rows()

}

problem_3_df <- results %>% 
  tibble(
    results
  ) %>% 
  mutate(
    mu = c(1, 2, 3, 4, 5, 6)
  ) %>% 
  unnest(results) %>% 
  select(mu, estimate, p.value)
```


### Plot showing the proportion of times the null was rejected

```{r}
problem_3_df %>%
  mutate(
    null_rejected = ifelse(p.value < 0.05, 1, 0)
  ) %>% 
  group_by(mu) %>%
  mutate(
    power = mean(null_rejected)
  ) %>% 
  ggplot(aes(x = mu, y = power)) + 
  geom_point() +
  labs(
    title = "Proportion of Times the Null is Rejected \n vs. True Value of Mu",
    x = "True value of mu",
    y = "Power of the Test"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

**Association Statement**: As the true value of mu (effect size) increases, the proportion of times the null is rejected (power of the test) also increases. 


### Plot comparing the average estimate of mu and the true value of mu

```{r, message = FALSE}
avg_v_true <- problem_3_df %>%
  group_by(mu) %>% 
  summarize(avg_mu = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = avg_mu)) +
  geom_point(alpha = 0.5, color = "black") +
  stat_smooth(geom = 'line', alpha = 0.5, color = "black") +
  labs(
    title = "When Null Is Not Rejected",
    x = "True value of mu",
    y = "Average estimated mu"
  ) +
  theme(plot.title = element_text(size = 14, hjust = 0.5))

avg_v_true_rejected = problem_3_df %>%
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(avg_mu = mean(estimate)) %>% 
  ggplot(aes(x = mu, y = avg_mu)) +
  geom_point(alpha = 0.5, color = "red",
             show.legend = TRUE) +
  stat_smooth(geom = 'line', alpha = 0.5, color = "red") +
  labs(
    title = "When Null Is Rejected",
    x = "True value of mu",
    y = "Average estimated mu"
  ) +
  theme(plot.title = element_text(size = 14, hjust = 0.5))

avg_v_true + avg_v_true_rejected +
  plot_annotation(
    title = "Average Estimated Mu Compared to the True Value of Mu",
    theme = theme(plot.title = element_text(size = 16, hjust = 0.5))
  )
```

The sample average of mu when the null is rejected does not approximate the true value of mu when the true value of mu equals 1 to 3. However, as mu increases (i.e., mu is equal to 4, 5, or 6), the sample average of mu when the null is rejected begins to better approximate the true value of mu. This trend aligns with the plot showing the proportion of times the null was rejected. Once mu = 4 or higher, the power of the test was at or around 1.0. This further supports that as effect size increases, power increases, and as effect size increases, the sample average of mu when the null is rejected better approximates the true value of mu. 
