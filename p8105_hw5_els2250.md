Homework 5
================
Emma Sexton <br>
Due 16 November 2022

## Problem 1

#### Import data using `list.files`

``` r
long_study_df =
  tibble(
    file_names = list.files(path = "data/problem_1/", all.files = TRUE, no.. = TRUE),
    path = str_c("data/problem_1/", file_names)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

#### Tidy data

``` r
tidy_long_df = 
  long_study_df %>% 
  mutate(
    files = str_replace(file_names, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>%
  select(group, subject = files, week, outcome)
```

#### Plot showing observations on each subject over time

``` r
tidy_long_df %>% 
  ggplot(aes(x = week, y = outcome, group = subject, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

<img src="p8105_hw5_els2250_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

There appears to be a positive trend in the experimental group where
outcome values increase as time passes. This trend is not observed or is
not as apparent in the control group. Additionally, the values in Week 8
are higher in the experimental group compared to the control, while
values were similar in both groups in Week 1.

## Problem 2

#### Importing and describing the data

``` r
homicide_df <- 
  read_csv(
    'data/problem_2/homicide-data.csv') %>%  
  janitor::clean_names()
```

Describe the raw data. Create a city_state variable (e.g. “Baltimore,
MD”) and then summarize within cities to obtain the total number of
homicides and the number of unsolved homicides (those for which the
disposition is “Closed without arrest” or “Open/No arrest”).

Describe the Data: ***need to do***

#### Summarizing total number of homicides and number of unsolved homicides

``` r
homicide_summary <- 
  homicide_df %>% 
  mutate(
    city_state = str_c(city, sep = ", ", state),
  ) %>% 
  group_by(city_state) %>% 
  summarise(
    n_homs = n(),
    n_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  ) %>% 
  arrange(desc(n_homs))

homicide_summary %>% 
  knitr::kable(col.names = c("City, State", "Total Homicides (n)", "Unsolved Homicides(n)"), 
               caption = "Number of Homicides per City, State (in descending order)")
```

| City, State        | Total Homicides (n) | Unsolved Homicides(n) |
|:-------------------|--------------------:|----------------------:|
| Chicago, IL        |                5535 |                  4073 |
| Philadelphia, PA   |                3037 |                  1360 |
| Houston, TX        |                2942 |                  1493 |
| Baltimore, MD      |                2827 |                  1825 |
| Detroit, MI        |                2519 |                  1482 |
| Los Angeles, CA    |                2257 |                  1106 |
| St. Louis, MO      |                1677 |                   905 |
| Dallas, TX         |                1567 |                   754 |
| Memphis, TN        |                1514 |                   483 |
| New Orleans, LA    |                1434 |                   930 |
| Las Vegas, NV      |                1381 |                   572 |
| Washington, DC     |                1345 |                   589 |
| Indianapolis, IN   |                1322 |                   594 |
| Kansas City, MO    |                1190 |                   486 |
| Jacksonville, FL   |                1168 |                   597 |
| Milwaukee, wI      |                1115 |                   403 |
| Columbus, OH       |                1084 |                   575 |
| Atlanta, GA        |                 973 |                   373 |
| Oakland, CA        |                 947 |                   508 |
| Phoenix, AZ        |                 914 |                   504 |
| San Antonio, TX    |                 833 |                   357 |
| Birmingham, AL     |                 800 |                   347 |
| Nashville, TN      |                 767 |                   278 |
| Miami, FL          |                 744 |                   450 |
| Cincinnati, OH     |                 694 |                   309 |
| Charlotte, NC      |                 687 |                   206 |
| Oklahoma City, OK  |                 672 |                   326 |
| San Francisco, CA  |                 663 |                   336 |
| Pittsburgh, PA     |                 631 |                   337 |
| New York, NY       |                 627 |                   243 |
| Boston, MA         |                 614 |                   310 |
| Tulsa, OK          |                 583 |                   193 |
| Louisville, KY     |                 576 |                   261 |
| Fort Worth, TX     |                 549 |                   255 |
| Buffalo, NY        |                 521 |                   319 |
| Fresno, CA         |                 487 |                   169 |
| San Diego, CA      |                 461 |                   175 |
| Stockton, CA       |                 444 |                   266 |
| Richmond, VA       |                 429 |                   113 |
| Baton Rouge, LA    |                 424 |                   196 |
| Omaha, NE          |                 409 |                   169 |
| Albuquerque, NM    |                 378 |                   146 |
| Long Beach, CA     |                 378 |                   156 |
| Sacramento, CA     |                 376 |                   139 |
| Minneapolis, MN    |                 366 |                   187 |
| Denver, CO         |                 312 |                   169 |
| Durham, NC         |                 276 |                   101 |
| San Bernardino, CA |                 275 |                   170 |
| Savannah, GA       |                 246 |                   115 |
| Tampa, FL          |                 208 |                    95 |
| Tulsa, AL          |                   1 |                     0 |

Number of Homicides per City, State (in descending order)

#### Estimating proportions of unsolved homicides: Running `prop.test` for Baltimore, MD

For the city of Baltimore, MD, use the `prop.test` function to estimate
the proportion of homicides that are unsolved; save the output of
prop.test as an R object, apply the broom::tidy to this object and pull
the estimated proportion and confidence intervals from the resulting
tidy dataframe.

``` r
baltimore_hom <-
  homicide_summary %>% 
  filter(city_state == "Baltimore, MD")

baltimore_prop <-
  prop.test(
    x = baltimore_hom$n_unsolved, 
    n = baltimore_hom$n_homs
  ) %>% 
  broom::tidy() %>% 
  select(estimate, starts_with("conf"))

baltimore_prop %>% 
  knitr::kable(digits = 3, 
               col.names = c("Estimate", "Lower CI", "Upper CI"), 
               caption = "Estimated Proportion and Confidence Interval (CI) for Unsolved Homicides in Baltimore, MD")
```

| Estimate | Lower CI | Upper CI |
|---------:|---------:|---------:|
|    0.646 |    0.628 |    0.663 |

Estimated Proportion and Confidence Interval (CI) for Unsolved Homicides
in Baltimore, MD

#### Estimating Proportions of unsolved homicides: Running `prop.test` for each city

Now run `prop.test` for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each. Do this within a “tidy” pipeline, making use of `purrr::map`,
`purrr::map2`, list columns and unnest as necessary to create a tidy
dataframe with estimated proportions and CIs for each city.

``` r
city_state_hom_prop <- homicide_summary %>% 
  mutate(
    results = map2(.x = homicide_summary$n_unsolved, 
                   .y = homicide_summary$n_homs, 
                   ~prop.test(x = .x, n = .y, conf.level = 0.95)),
    results = map(.x = results, 
                  ~broom::tidy(.x))
    ) %>% 
  unnest(results) %>% 
  select(city_state, estimate, starts_with("conf")) %>% 
  arrange(desc(estimate))

city_state_hom_prop %>% 
  knitr::kable(digits = 3, 
               col.names = c("City, State", "Estimate", "Lower CI", "Upper CI"), 
               caption = "Estimated Proportion and Confidence Intervals (CI) for Unsolved Homicides across the United States (in descending order)")
```

| City, State        | Estimate | Lower CI | Upper CI |
|:-------------------|---------:|---------:|---------:|
| Chicago, IL        |    0.736 |    0.724 |    0.747 |
| New Orleans, LA    |    0.649 |    0.623 |    0.673 |
| Baltimore, MD      |    0.646 |    0.628 |    0.663 |
| San Bernardino, CA |    0.618 |    0.558 |    0.675 |
| Buffalo, NY        |    0.612 |    0.569 |    0.654 |
| Miami, FL          |    0.605 |    0.569 |    0.640 |
| Stockton, CA       |    0.599 |    0.552 |    0.645 |
| Detroit, MI        |    0.588 |    0.569 |    0.608 |
| Phoenix, AZ        |    0.551 |    0.518 |    0.584 |
| Denver, CO         |    0.542 |    0.485 |    0.598 |
| St. Louis, MO      |    0.540 |    0.515 |    0.564 |
| Oakland, CA        |    0.536 |    0.504 |    0.569 |
| Pittsburgh, PA     |    0.534 |    0.494 |    0.573 |
| Columbus, OH       |    0.530 |    0.500 |    0.560 |
| Jacksonville, FL   |    0.511 |    0.482 |    0.540 |
| Minneapolis, MN    |    0.511 |    0.459 |    0.563 |
| Houston, TX        |    0.507 |    0.489 |    0.526 |
| San Francisco, CA  |    0.507 |    0.468 |    0.545 |
| Boston, MA         |    0.505 |    0.465 |    0.545 |
| Los Angeles, CA    |    0.490 |    0.469 |    0.511 |
| Oklahoma City, OK  |    0.485 |    0.447 |    0.524 |
| Dallas, TX         |    0.481 |    0.456 |    0.506 |
| Savannah, GA       |    0.467 |    0.404 |    0.532 |
| Fort Worth, TX     |    0.464 |    0.422 |    0.507 |
| Baton Rouge, LA    |    0.462 |    0.414 |    0.511 |
| Tampa, FL          |    0.457 |    0.388 |    0.527 |
| Louisville, KY     |    0.453 |    0.412 |    0.495 |
| Indianapolis, IN   |    0.449 |    0.422 |    0.477 |
| Philadelphia, PA   |    0.448 |    0.430 |    0.466 |
| Cincinnati, OH     |    0.445 |    0.408 |    0.483 |
| Washington, DC     |    0.438 |    0.411 |    0.465 |
| Birmingham, AL     |    0.434 |    0.399 |    0.469 |
| San Antonio, TX    |    0.429 |    0.395 |    0.463 |
| Las Vegas, NV      |    0.414 |    0.388 |    0.441 |
| Omaha, NE          |    0.413 |    0.365 |    0.463 |
| Long Beach, CA     |    0.413 |    0.363 |    0.464 |
| Kansas City, MO    |    0.408 |    0.380 |    0.437 |
| New York, NY       |    0.388 |    0.349 |    0.427 |
| Albuquerque, NM    |    0.386 |    0.337 |    0.438 |
| Atlanta, GA        |    0.383 |    0.353 |    0.415 |
| San Diego, CA      |    0.380 |    0.335 |    0.426 |
| Sacramento, CA     |    0.370 |    0.321 |    0.421 |
| Durham, NC         |    0.366 |    0.310 |    0.426 |
| Nashville, TN      |    0.362 |    0.329 |    0.398 |
| Milwaukee, wI      |    0.361 |    0.333 |    0.391 |
| Fresno, CA         |    0.347 |    0.305 |    0.391 |
| Tulsa, OK          |    0.331 |    0.293 |    0.371 |
| Memphis, TN        |    0.319 |    0.296 |    0.343 |
| Charlotte, NC      |    0.300 |    0.266 |    0.336 |
| Richmond, VA       |    0.263 |    0.223 |    0.308 |
| Tulsa, AL          |    0.000 |    0.000 |    0.945 |

Estimated Proportion and Confidence Intervals (CI) for Unsolved
Homicides across the United States (in descending order)

#### Plot of unsolved homicide proportions and confidence intervals for each city

Create a plot that shows the estimates and CIs for each city – check out
geom_errorbar for a way to add error bars based on the upper and lower
limits. Organize cities according to the proportion of unsolved
homicides.

## Problem 3

#### Conducting a simulation to explore power in a one-sample t-test

First, we need to set the following design elements:

``` r
# sim_model <- function(sample_n = 30, mu, sigma = 5) {
#   
#   model_df = 
#     tibble(
#       x = rnorm(n = sample_n, mean = mu, sd = sigma),
#     )
#   
#   model_df %>% 
#     t.test(., conf.level = 0.95) %>% 
#     broom::tidy() %>% 
#     select(estimate, p.value)
#   
# }
# 
# problem_3_dataset <- 
#   5000 %>% 
#   rerun(sim_model(mu = 0))
```

A dataframe consisting of 5000 datasets, where each list contains the
estimate value and the p-value for that dataset were created above.

Next, we’ll repeat this when mu = {1 ,2, 3, 4, 5, 6}:

``` r
# mu_list = 
#   list(
#     "mu = 1" = 1
#     
#   )
# 
# output = vector("list", length = 6)
# 
# output[[1]] = sim_model(mu_list[[1]])
# 
# for(i in 1:6) {
#   
#   output[[i]] = mean_and_sd(list_norm[[i]])
# 
# }
```
